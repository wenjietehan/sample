{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
   "metadata": {},
   "source": [
    "# Using Multiple Models\n",
    "\n",
    "This notebook contains code to do the following:\n",
    "\n",
    "* Convert Python code to C++\n",
    "\n",
    "* Add docstrings or comments to Python code\n",
    "  \n",
    "* Create Unit tests\n",
    "\n",
    "* Buy/Sell Equatities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "\n",
    "#import io\n",
    "#import sys\n",
    "#import json\n",
    "#import requests\n",
    "\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import ollama\n",
    "\n",
    "#import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f672e1c-87e9-4865-b760-370fa605e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa149ed-9298-4d69-8fe2-8f5de0f667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLMs\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "CLAUDE_MODEL = \"claude-3-haiku-20240307\"\n",
    "GEMINI_MODEL = \"gemini-1.5-flash\"\n",
    "OLLAMA_MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e709137f-2d97-4e6c-b243-fcb853bc627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Class\n",
    "class MessageHandler:\n",
    "    def __init__(self, system_message, user_prompt, python_code):\n",
    "        self.system_message = system_message\n",
    "        self.user_prompt = user_prompt\n",
    "        self.user_prompt_for = user_prompt + \"\\n\" + python_code\n",
    "        self.messages_for = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": self.user_prompt_for}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c73b8a-d1e9-4f72-81b4-702ef13a3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Prompt\n",
    "def get_prompt(python_code, feature):\n",
    "    system_message = \"\"\n",
    "    user_prompt = \"\"\n",
    "    if feature==\"Code Conversion\":\n",
    "        system_message = \"You are an assistant that reimplements Python code in high performance C++ for an M4 Mac. \"\n",
    "        system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
    "        system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\"\n",
    "        \n",
    "        user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
    "        user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
    "        user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
    "    \n",
    "    elif feature==\"Add Comments\":\n",
    "        system_message = \"You are an assistant specialized in adding clear and concise docstrings and comments to Python code to enhance its readability and maintainability. \"\n",
    "        system_message += \"Respond exclusively using valid Python comments and docstrings.\"\n",
    "                \n",
    "        user_prompt = \"Add appropriate comments and docstrings to the Python code provided below. Do not modify the code itself. \"\n",
    "        user_prompt += \"Avoid summarizing or providing notes on your additions. Place comments for parameters outside the parentheses. \"\n",
    "        user_prompt += \"Any output occurring before the first import statement should be enclosed within comments. Do not output the word 'python' at the top.\"\n",
    "        user_prompt += \"Return the modified code with the added explanations and documentation.\\n\\n\"\n",
    "    \n",
    "    elif feature==\"Generate Unit Tests\":\n",
    "        system_message = \"You are an assistant specialized in generating unit tests for Python code to ensure its accuracy and reliability. \"\n",
    "        system_message += \"Respond only with valid Python code.\"\n",
    "                \n",
    "        user_prompt = \"Create unit tests for the Python code provided below. \"\n",
    "        user_prompt += \"Return only the unit test code. Do not output the word 'python' at the top.\\n\\n\"\n",
    "    \n",
    "    elif feature==\"Generate Trading Bot Code\":\n",
    "        system_message = \"You are a Python developer specializing in creating trading bots for buying and selling equities. \"\n",
    "        system_message += \"Write accurate and functional Python code for executing equity trades.\"\n",
    "                \n",
    "        user_prompt = \"Write Python code to buy and sell equities using the Alpaca API. \"\n",
    "        user_prompt += \"Provide only the Python code as the output. Do not output the word 'python' at the top.\\n\\n\"\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown Use Case\")\n",
    "\n",
    "    prompt = MessageHandler(system_message, user_prompt, python_code)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ca748-151f-4c8d-987e-d919d0412ce2",
   "metadata": {},
   "source": [
    "## Streaming output with different LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):    \n",
    "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=prompt.messages_for, stream=True)\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk.choices[0].delta.content or \"\"\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')\n",
    "\n",
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model=CLAUDE_MODEL,\n",
    "        max_tokens=2000,\n",
    "        system=prompt.system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt.user_prompt_for}],\n",
    "    )\n",
    "    reply = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            reply += text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name=GEMINI_MODEL,\n",
    "        system_instruction=prompt.system_message\n",
    "    )\n",
    "    \n",
    "    response = gemini.generate_content(\n",
    "        prompt.user_prompt_for,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    reply = \"\"\n",
    "    # Process the stream\n",
    "    for chunk in response:\n",
    "        # Extract text from the chunk\n",
    "        if chunk.text:\n",
    "            reply += chunk.text\n",
    "            yield reply.replace('```cpp\\n','').replace('```','')\n",
    "\n",
    "def stream_Llama(prompt):\n",
    "    stream = ollama.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=prompt.messages_for,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    reply = \"\"\n",
    "    for chunk in stream:\n",
    "        fragment = chunk[\"message\"][\"content\"]\n",
    "        reply += fragment\n",
    "        yield reply.replace('```cpp\\n','').replace('```','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40de95f-0ed7-4094-b275-e0fe4e0cba1e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d72f33-e6a7-46b7-87e6-fa174149fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(python, model, feature):\n",
    "    if feature==\"Code Conversion\":\n",
    "        prompt = get_prompt(python, feature)\n",
    "    elif feature==\"Add Comments\":\n",
    "        prompt = get_prompt(python, feature)\n",
    "    elif feature==\"Generate Unit Tests\":\n",
    "        prompt = get_prompt(python, feature)    \n",
    "    elif feature==\"Generate Trading Bot Code\":\n",
    "        prompt = get_prompt(python, feature) \n",
    "    else:\n",
    "        raise ValueError(\"Unknown Use Case\")\n",
    "\n",
    "    if model==\"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model==\"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    elif model==\"Gemini\":\n",
    "        result = stream_gemini(prompt)\n",
    "    elif model==\"Llama\":\n",
    "        result = stream_Llama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Model\")\n",
    "    \n",
    "    for stream_so_far in result:\n",
    "        yield stream_so_far   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1303932-160c-424b-97a8-d28c816721b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css = \"\"\"\n",
    ".python {background-color: #ADD8E6;}\n",
    ".output {background-color: #fefbd9;}\n",
    "\"\"\"\n",
    "\n",
    "def read_input(user_input):\n",
    "    return f\"You entered: {user_input}\"\n",
    "\n",
    "with gr.Blocks(css=css) as ui:\n",
    "    gr.Markdown(\"## Multi-model Multi-features Exercise\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            fn = read_input,\n",
    "            python = gr.Textbox(label=\"Enter Python Code Here:\", lines=10, elem_classes=[\"python\"])\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                model = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\", \"Llama\"], label=\"Select model\", value=\"GPT\")\n",
    "            with gr.Row():\n",
    "                feature = gr.Dropdown([\"Code Conversion\", \"Add Comments\", \"Generate Unit Tests\", \"Generate Trading Bot Code\"], label=\"Select Use Case\", value=\"Code Conversion\") \n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"Output:\", lines=10, elem_classes=[\"output\"])\n",
    "    with gr.Row():\n",
    "        run = gr.Button(\"Run\")\n",
    "    \n",
    "    run.click(get_results, inputs=[python, model, feature], outputs=[output])\n",
    "\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20226ac2-c24d-44ee-8a83-6cdb12872347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
